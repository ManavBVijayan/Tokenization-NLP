{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59922175-dafb-44c6-b39c-5a1585176418",
   "metadata": {},
   "source": [
    "##### word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c449af22-5619-466a-a8f5-0c7731e55073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Tokenization is an important step in natural language processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781c58ee-5624-4d52-bf8f-ab6d905fa220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b57096-0ae3-4b95-acbd-cab3b6f8121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31f2d5a-3e37-465f-a49a-c3627eb897bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokenization ['Tokenization', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens=word_tokenize(text)\n",
    "print(\"Word tokenization\",tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68afc0d3-d691-4b32-8b81-a0926168175f",
   "metadata": {},
   "source": [
    "##### Senetence Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e2e960-5ddf-4148-badc-d4213e5a11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e689a9b9-82d9-4637-9e4b-bf8a4f91253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenization is an important step. It helps in natural language processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ed74871-ba43-45e6-9b79-8ceebcdb7830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization ['Tokenization is an important step.', 'It helps in natural language processing.']\n"
     ]
    }
   ],
   "source": [
    "tokens=sent_tokenize(text)\n",
    "print(\"Sentence Tokenization\",tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7bad2f-6b6a-4b6d-ad37-17865c28c4ab",
   "metadata": {},
   "source": [
    "##### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d8d2ef3-0bc4-40ec-a92c-978675e7788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokenization: ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't', ' ', 's', 't', 'e', 'p', ' ', 'i', 'n', ' ', 'n', 'a', 't', 'u', 'r', 'a', 'l', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Tokenization is an important step in natural language processing.\"\n",
    "char_tokens = list(text)\n",
    "print(\"Character Tokenization:\", char_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a5918-6261-48b8-86b1-67e909305240",
   "metadata": {},
   "source": [
    "##### Regular Expression Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d42b4f-f08f-4689-8ff8-ddf2b81bb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23ac4dab-b9c2-49a2-b7c8-828944468503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Tokenization is an important step in natural language processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa98a64d-bda6-47f8-a713-63198652d86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern for tokenization\n",
    "pattern = r\"\\w+|\\$[\\d\\.]+|\\S+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f05810d-2b43-4e15-9064-dffc15b7038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f592c056-1095-4e90-87b8-7a1d013c56ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Expression Tokenization ['Tokenization', 'is', 'an', 'important', 'step', 'in', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Regular Expression Tokenization\",tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44093e34-93ee-4b27-aa11-607760c0bffa",
   "metadata": {},
   "source": [
    "##### White Space Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "270e16e3-33c6-4c8f-8e5a-85007023a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace Tokenization: ['Whitespace', 'tokenization', 'is', 'a', 'basic', 'method', 'for', 'splitting', 'text.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Whitespace tokenization is a basic method for splitting text.\"\n",
    "\n",
    "whitespace_tokens = text.split()\n",
    "\n",
    "print(\"Whitespace Tokenization:\", whitespace_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
